---
layout: post
title: "R and Spark"
status: wait
published: false
---

## sparklyr

* Source: http://spark.rstudio.com/ 

### Installation
* Install sparklyr
```
install.packages("sparklyr")
```

* Install spark
```
## to install spark 2.0.1, hadoop 2.7
library(sparklyr)
spark_install(version = "1.6.2")


## to install latest version
tmp <- spark_available_versions()
eval(parse(text = tmp[tmp$spark == max(tmp$spark) & tmp$hadoop == max(tmp$hadoop[tmp$hadoop != "cdh4"]), "install"]))
```

### Create connection to spark
```
sc <- spark_connect(master = "local")
```

* View Spark web UI
```
spark_web(sc)
```

* View Spark log
```
spark_log(sc)
```

### Disconnect to spark
```
spark_disconnect(sc)
```

## SparkR

* Source: http://ampcamp.berkeley.edu/5/exercises/sparkr.html

### Installation
* Install R
* rJava
```
install.packages("rJava")
library(rJava)
```
* Install SparkR (which is Spark): the easiest way is to use the sparklyr package (as above)

### Create the spark context
```
library(SparkR)
Sys.setenv(SPARK_MEM="1g")
sc <- sparkR.init(master="local[*]")
```
